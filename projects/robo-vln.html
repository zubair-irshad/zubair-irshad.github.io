<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<!-- ======================================================================= -->
<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>
<style type="text/css">
  body {
    font-family: "Titillium Web","HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight:300;
    font-size:18px;
    display: flex;
    justify-content: center;
    flex-direction:column;
  }

  @media screen and (max-width: 800px){
    body {
     margin-left: auto;
     margin-right: auto; 
     width: 1100px;
     }
   }

  h1 {
    font-weight:300;
  }

  .disclaimerbox {
    background-color: #eee;
    border: 1px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
    padding: 20px;
  }

  video.header-vid {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.header-img {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.rounded {
    border: 1px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  a:link,a:visited
  {
    color: #1367a7;
    text-decoration: none;
  }
  a:hover {
    color: #208799;
  }

  td.dl-link {
    height: 160px;
    text-align: center;
    font-size: 22px;
  }

  .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
            15px 15px 0 0px #fff, /* The fourth layer */
            15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
            20px 20px 0 0px #fff, /* The fifth layer */
            20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
            25px 25px 0 0px #fff, /* The fifth layer */
            25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
    margin-left: 10px;
    margin-right: 45px;
  }


  .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
    margin-top: 5px;
    margin-left: 10px;
    margin-right: 30px;
    margin-bottom: 5px;
  }

  .vert-cent {
    position: relative;
      top: 50%;
      transform: translateY(-50%);
  }

  .hr
  {
    border: 0;
    height: 1px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
  }

  #authors td {
    padding-bottom:5px;
    padding-top:30px;
  }
</style>
<!-- ======================================================================= -->

<!-- Start : Google Analytics Code -->
<!-- <script async src="https://www.googletagmanager.com/gtag/js?id=UA-64069893-4"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-64069893-4');
</script> -->
<!-- End : Google Analytics Code -->

<script type="text/javascript" src="resources/hidebib.js"></script>
<link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>

<!-- Mirrored from pathak22.github.io/modular-assemblies/ by HTTrack Website Copier/3.x [XR&CO'2014], Thu, 15 Apr 2021 23:40:29 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Hierarchical Cross-Modal Agent for Robotics Vision-and-Language Navigation</title>
  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <link rel="canonical" href="index.html" />
  <meta name="referrer" content="no-referrer-when-downgrade" />

  <meta property="og:site_name" content="Self-Assembling Morphologies" />
  <meta property="og:type" content="video.other" />
  <meta property="og:title" content="Hierarchical Cross-Modal Agent for Robotics Vision-and-Language Navigation" />
  <meta property="og:description" content="Irshad M. Zubair, Ma Chih-Yao, Kira Zsolt. Hierarchical Cross-Modal Agent for Robotics Vision-and-Language Navigation. ICRA 2021." />
  <meta property="og:url" content="https://pathak22.github.io/modular-assemblies/" />
  <meta property="og:image" content="https://pathak22.github.io/modular-assemblies/resources/teaser.jpg" />
  <meta property="og:video" content="https://www.youtube.com/v/ngCIB-IWD8E" />

  <meta property="article:publisher" content="http://people.eecs.berkeley.edu/~pathak/" />
  <meta name="twitter:card" content="summary_large_image" />
  <meta name="twitter:title" content="Learning to Control Self-Assembling Morphologies: A Study of Generalization via Modularity" />
  <meta name="twitter:description" content="Pathak, Lu, Darrell, Isola, Efros. Learning to Control Self-Assembling Morphologies: A Study of Generalization via Modularity. 2019." />
  <meta name="twitter:url" content="https://pathak22.github.io/modular-assemblies/" />
  <meta name="twitter:image" content="https://pathak22.github.io/modular-assemblies/resources/teaser.jpg" />
  <meta name="twitter:label1" content="Written by" />
  <meta name="twitter:data1" content="Deepak Pathak" />
  <meta name="twitter:label2" content="Filed under" />
  <meta name="twitter:data2" content="" />
  <meta name="twitter:site" content="@pathak" />
  <meta property="og:image:width" content="1600" />
  <meta property="og:image:height" content="900" />

  <script src="https://www.youtube.com/iframe_api"></script>
  <meta name="twitter:card" content="player" />
  <meta name="twitter:image" content="https://pathak22.github.io/modular-assemblies/resources/teaser.jpg" />
  <meta name="twitter:player" content="https://www.youtube.com/embed/ngCIB-IWD8E?rel=0&showinfo=0" />
  <meta name="twitter:player:width" content="640" />
  <meta name="twitter:player:height" content="360" />
</head>

<body>
      <br>
      <center><span style="font-size:44px;font-weight:bold;">Hierarchical Cross-Modal Agent for<br/>Robotics Vision-and-Language Navigation</span></center><br/>
      <table align=center width=800px>
      <tr>
        <td align=center width=150px>
        <center><span style="font-size:22px"><a href="https://zubairirshad.com" target="_blank">Muhammad Zubair Irshad</a></span></center></td>
        <td align=center width=150px>
        <center><span style="font-size:22px"><a href="https://chihyaoma.github.io/" target="_blank">Chih-Yao Ma</a></span></center></td>
        <td align=center width=150px>
        <center><span style="font-size:22px"><a href="https://www.cc.gatech.edu/~zk15/" target="_blank">Zsolt Kira</a></span></center></td>
      <tr/>
      <tr>
        <td align=center width=200px>
        <center><span style="font-size:20px">Georgia Tech</span></center></td>
        <td align=center width=200px>
        <center><span style="font-size:20px">FAIR/Georgia Tech</span></center></td>
        <td align=center width=200px>
        <center><span style="font-size:20px">Georgia Tech</span></center></td>
      <tr/>
      <table align=center width=700px>
          <tr>
            <td align=center width=700px><center><span style="font-size:22px">Accepted at <a href="http://www.icra2021.org/">IEEE International Conference on Robotics and Automation (ICRA), 2021</a></span></center></td>
          <tr/>
      </table><br/>
      <table align=center width=700px>
          <tr>
            <td align=center width=100px><center><span style="font-size:28px"><a href="resources/HCM_ICRA21.pdf">[Paper]</a></span></center></td>
            <td align=center width=100px><center><span style="font-size:28px"><a href="resources/slides.pdf">[Dataset]</a></span></center></td>
            <td align=center width=100px><center><span style="font-size:28px"><a href="resources/poster.pdf">[Poster]</a></span></center></td>
            <td align=center width=100px><center><span style="font-size:28px"><a href='https://github.com/pathak22/modular-assemblies/'>[GitHub Code]</a></span></center></td>
          <tr/>
      </table><br/>

<!--       <center><h2>Project Video</h2></center> -->
      <table align=center width=1000px>
        <p style="margin-top:4px;"></p>
        <tr><td width=1000px>
          <center><a href="resources/ACMI_final.svg"><img src = "resources/ACMI_final.svg" height="260px"></img></a><br></center>
        </td></tr>
      </table>
      <br>

      <div style="width:800px; margin:0 auto; text-align=center">
        Deep Learning has revolutionized our ability to solve complex problems such as Vision-and-Language Navigation (VLN). This task requires the agent to navigate to a goal purely based on visual sensory inputs given natural language instructions. However, prior works formulate the problem as a navigation graph with a discrete action space. In this work, we lift the agent off the navigation graph and propose a more complex VLN setting in continuous 3D reconstructed environments. Our proposed setting, Robo-VLN, more closely mimics the challenges of real world navigation. Robo-VLN tasks have longer trajectory lengths, continuous action spaces, and challenges such as obstacles. We provide a suite of baselines inspired by state-of-the-art works in discrete VLN and show that they are less effective at this task. We further propose that decomposing the task into specialized high- and low-level policies can more effectively tackle this task. With extensive experiments, we show that by using layered decision making, modularized training, and decoupling reasoning and imitation, our proposed Hierarchical Cross-Modal (HCM) agent outperforms existing baselines in all key metrics and sets a new benchmark for Robo-VLN.      </div>
      <br><span class=hr></span>
      
      <center><h1>Robo-VLN Dataset</h1></center>
      <div style="width:800px; margin:0 auto; text-align=center">
        Different from existing VLN environments, we propose a new continuous environment for VLN that more closely mirrors the challenges of the real world, Robo-VLN --- a continuous control formulation for Vision-and-Language Navigation. Compared to navigation graph based and discrete VLN settings, Robo-VLN provides longer horizon trajectories (4.5x average number of steps), more visual frames (3.5M visual frames), and a balanced high-level action distribution. Hence, making the problem more challenging and closer to the real-world.
        Try our Dataset!
      </div>
      <table align=center width=1000px>
        <p style="margin-top:4px;"></p>
        <tr><td width=1000px>
          <center><a href="resources/robo-vln-dataset.gif"><img src = "resources/robo-vln-dataset.gif" height="260px"></img></a><br></center>
        </td></tr>
      </table>

      <table align=center width=900px>
        <tr>
          <!-- <p style="margin-top:4px;"></p> -->
          <td width=300px align=center>
            <span style="font-size:28px"><a href='https://github.com/pathak22/modular-assemblies/'>[GitHub]</a></span>
          </td>
        </tr>
      </table>
      <br><span class=hr></span>



      <center><h1>Demo Video</h1></center><br/>
      <table align=center width=560px>
        <tr><td align=center width=560px>
          <iframe width="560" height="315" src="https://www.youtube.com/embed/y16x9n_zP_4" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
        </td></tr>
        </table>
      <br/><span class=hr></span>

      <center id="sourceCode"><h1>Source Code and Environment</h1></center>
      <div style="width:800px; margin:0 auto; text-align=center">
      We have released the PyTorch based implementation and environment on the github page. Try our code!
      </div>
      <table align=center width=900px>
        <tr>
          <!-- <p style="margin-top:4px;"></p> -->
          <td width=300px align=center>
            <span style="font-size:28px"><a href='https://github.com/pathak22/modular-assemblies/'>[GitHub]</a></span>
          </td>
        </tr>
      </table>
      <br><span class=hr></span>

      <table align=center width=850px>
        <center><h1>Paper and Bibtex</h1></center>
        <tr>
        <td width:200px align=left>
        <!-- <p style="margin-top:4px;"></p> -->
        <a href="resources/HCM_ICRA21.pdf"><img style="width:200px" src="resources/thumbnail_hcm.jpg"/></a>
        <center>
        <span style="font-size:20pt"><a href="resources/HCM_ICRA21.pdf">[Paper]</a>
        <span style="font-size:20pt"><a href="https://arxiv.org/abs/1902.05546v2">[ArXiv]</a>
        </center>
        </td>
        <td width=50px align=center>
        </td>
        <td width=550px align=left>
        <!-- <p style="margin-top:4px;"></p> -->
        <p style="text-align:left;"><b><span style="font-size:20pt">Citation</span></b><br/><span style="font-size:6px;">&nbsp;<br/></span> <span style="font-size:15pt"> M.Z. Irshad, C.Y. Ma, and Z. Kira. <b>Hierarchical Cross-Modal Agent for Robotics Vision-and-Language Navigation<br/></b> In <i>International Conference on Robotics and Automation (ICRA)</i> 2021.</span></p>
        <!-- <p style="margin-top:20px;"></p> -->
        <span style="font-size:20pt"><a shape="rect" href="javascript:togglebib('assemblies19_bib')" class="togglebib">[Bibtex]</a></span>
        </td>
        </tr>
        <tr>
        <td width=250px align=left>
        </td>
        <td width=50px align=center>
        </td>
        <td width=550px align=left>
          <div class="paper" id="assemblies19_bib">
<pre xml:space="preserve">
@inproceedings{pathak19assemblies,
  Author = {Pathak, Deepak and
  Lu, Chris and Darrell, Trevor and
  Isola, Phillip and Efros, Alexei A.},
  Title = {Learning to Control Self-
  Assembling Morphologies: A Study of
  Generalization via Modularity},
  Booktitle = {NeurIPS},
  Year = {2019}
}</pre>
          </div>
          </td>
          </tr>
      </table>
    <br>
    
    <!-- <span class=hr></span>

    <table align=center width=800px>
      <tr><td width=800px><left>
      <center><h1>Acknowledgements</h1></center>
      We would like to thank Igor Mordatch, Chris Atkeson, Abhinav Gupta and the members of BAIR for fruitful discussions and comments. This work was supported in part by Berkeley DeepDrive, and the Valrhona reinforcement learning fellowship. DP is supported by the Facebook graduate fellowship.<br>
      </left></td></tr>
    </table>
  <br><br> -->

<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>
</body>

<!-- Mirrored from pathak22.github.io/modular-assemblies/ by HTTrack Website Copier/3.x [XR&CO'2014], Thu, 15 Apr 2021 23:40:30 GMT -->
</html>
